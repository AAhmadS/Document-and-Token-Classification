{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "toc_visible": true,
      "authorship_tag": "ABX9TyOBdcTaOP1HL5/JJWa0X7s9",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/AAhmadS/NLP_HW4/blob/main/Sentiment_analysis.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#NLP Homework 4"
      ],
      "metadata": {
        "id": "6f77_QYUoCU5"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install -qU transformers\n",
        "!pip install -qU torchtext\n",
        "!pip install -qU fasttext"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "AVW9dUiqqC2x",
        "outputId": "f5c59b6c-a377-493a-8ad8-e3fdc4503bc9"
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m7.2/7.2 MB\u001b[0m \u001b[31m87.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m236.8/236.8 kB\u001b[0m \u001b[31m18.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m7.8/7.8 MB\u001b[0m \u001b[31m101.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.3/1.3 MB\u001b[0m \u001b[31m66.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m68.8/68.8 kB\u001b[0m \u001b[31m5.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Building wheel for fasttext (setup.py) ... \u001b[?25l\u001b[?25hdone\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!git clone https://github.com/facebookresearch/fastText.git"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "UiLCZAB2qsPC",
        "outputId": "5b7c44d6-3b83-421f-d993-6ca4c427a198"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Cloning into 'fastText'...\n",
            "remote: Enumerating objects: 3943, done.\u001b[K\n",
            "remote: Counting objects: 100% (1002/1002), done.\u001b[K\n",
            "remote: Compressing objects: 100% (151/151), done.\u001b[K\n",
            "remote: Total 3943 (delta 903), reused 859 (delta 850), pack-reused 2941\u001b[K\n",
            "Receiving objects: 100% (3943/3943), 8.25 MiB | 42.05 MiB/s, done.\n",
            "Resolving deltas: 100% (2510/2510), done.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import fasttext.util"
      ],
      "metadata": {
        "id": "3MbF6SqdsRvL"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!sudo pip install fastText/.\n",
        "!cd fastText\n",
        "fasttext.util.download_model('fa', if_exists='ignore')\n",
        "ft = fasttext.load_model('cc.fa.300.bin')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "8Z4hRGDiquyb",
        "outputId": "4af93510-5d2a-4046-8e07-f59b09d4be2e"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Processing ./fastText\n",
            "  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Requirement already satisfied: pybind11>=2.2 in /usr/local/lib/python3.10/dist-packages (from fasttext==0.9.2) (2.10.4)\n",
            "Requirement already satisfied: setuptools>=0.7.0 in /usr/local/lib/python3.10/dist-packages (from fasttext==0.9.2) (67.7.2)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.10/dist-packages (from fasttext==0.9.2) (1.22.4)\n",
            "Building wheels for collected packages: fasttext\n",
            "  Building wheel for fasttext (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for fasttext: filename=fasttext-0.9.2-cp310-cp310-linux_x86_64.whl size=4384288 sha256=f778e46d15aa1278d76a806135be813db3ee0d4ec7927fcf2373ca19229cc2db\n",
            "  Stored in directory: /tmp/pip-ephem-wheel-cache-njk8quld/wheels/8b/05/af/3cfae069d904597d44b309c956601b611bdf8967bcbe968903\n",
            "Successfully built fasttext\n",
            "Installing collected packages: fasttext\n",
            "  Attempting uninstall: fasttext\n",
            "    Found existing installation: fasttext 0.9.2\n",
            "    Uninstalling fasttext-0.9.2:\n",
            "      Successfully uninstalled fasttext-0.9.2\n",
            "Successfully installed fasttext-0.9.2\n",
            "Downloading https://dl.fbaipublicfiles.com/fasttext/vectors-crawl/cc.fa.300.bin.gz\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "u6e8tddyq1NL",
        "outputId": "3a329ab2-a791-4134-937b-ceb979d1adc0"
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "%cd /content/drive/MyDrive/nlp\n",
        "%mkdir nlp_hw4\n",
        "%cd nlp_hw4"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "8WTcBKFmq24T",
        "outputId": "dcb8c985-3866-4f42-d82e-94022d29be34"
      },
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[Errno 2] No such file or directory: '/content/drive/MyDrive/nlp'\n",
            "/content\n",
            "/content/nlp_hw4\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "##Essential packages"
      ],
      "metadata": {
        "id": "WVv2J05RoLoy"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "from numpy import pad\n",
        "import matplotlib.pyplot as plt\n",
        "plt.switch_backend('agg')\n",
        "%matplotlib inline\n",
        "import matplotlib.ticker as ticker\n",
        "from collections import Counter\n",
        "from __future__ import unicode_literals, print_function, division\n",
        "import random\n",
        "from tqdm import tqdm\n",
        "from sklearn.utils import resample\n",
        "from joblib import Parallel, delayed\n",
        "from sklearn import metrics\n",
        "from sklearn.model_selection import train_test_split as tts\n",
        "import sklearn.svm as svm\n",
        "from sklearn.model_selection import StratifiedKFold as skf\n",
        "from sklearn.metrics import roc_auc_score\n",
        "from sklearn.metrics import accuracy_score as acs\n",
        "import plotly.express as px\n",
        "from sklearn.metrics import roc_curve, auc,precision_recall_curve\n",
        "from sklearn.model_selection import GridSearchCV\n",
        "from sklearn.model_selection import RandomizedSearchCV\n",
        "from sklearn.datasets import make_classification\n",
        "from sklearn.preprocessing import MinMaxScaler\n",
        "from sklearn.model_selection import cross_val_score\n",
        "from sklearn.utils.fixes import loguniform\n",
        "from sklearn.metrics import f1_score\n",
        "from sklearn.model_selection import learning_curve\n",
        "from sklearn.metrics import classification_report\n",
        "import seaborn as sns\n",
        "import fasttext.util\n",
        "\n",
        "\n",
        "import torch\n",
        "from torch import nn\n",
        "import torchtext\n",
        "from torchtext.vocab import Vocab\n",
        "from torch.nn.utils.rnn import pad_sequence\n",
        "from torch.utils.data import DataLoader, Dataset, TensorDataset\n",
        "from torchtext.data.utils import get_tokenizer\n",
        "import torch.nn.functional as F\n",
        "from torch import optim\n",
        "\n",
        "import nltk\n",
        "from nltk.tokenize import sent_tokenize\n",
        "\n",
        "from transformers import pipeline, AutoTokenizer\n",
        "\n",
        "\n",
        "import plotly.express as px\n",
        "import plotly.graph_objects as go\n",
        "\n",
        "from tqdm.notebook import tqdm\n",
        "\n",
        "import os\n",
        "\n",
        "import copy\n",
        "import collections\n",
        "import torch"
      ],
      "metadata": {
        "id": "RH_5fv9ep_9J"
      },
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##Load data and analysis\n"
      ],
      "metadata": {
        "id": "2CCwlBiboO3F"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!unzip archive.zip\n",
        "\n",
        "df = pd.read_csv('data.csv')\n",
        "df.head(3)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 397
        },
        "id": "MRPeM6-mqfgi",
        "outputId": "82ebdc68-a399-4936-9cae-500e031163e4"
      },
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "unzip:  cannot find or open archive.zip, archive.zip.zip or archive.zip.ZIP.\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "FileNotFoundError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-9-ebed3eca0322>\u001b[0m in \u001b[0;36m<cell line: 3>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0mget_ipython\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msystem\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'unzip archive.zip'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 3\u001b[0;31m \u001b[0mdf\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mread_csv\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'data.csv'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      4\u001b[0m \u001b[0mdf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mhead\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m3\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/pandas/util/_decorators.py\u001b[0m in \u001b[0;36mwrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    209\u001b[0m                 \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    210\u001b[0m                     \u001b[0mkwargs\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mnew_arg_name\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnew_arg_value\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 211\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    212\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    213\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mcast\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mF\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mwrapper\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/pandas/util/_decorators.py\u001b[0m in \u001b[0;36mwrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    329\u001b[0m                     \u001b[0mstacklevel\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mfind_stack_level\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    330\u001b[0m                 )\n\u001b[0;32m--> 331\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    332\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    333\u001b[0m         \u001b[0;31m# error: \"Callable[[VarArg(Any), KwArg(Any)], Any]\" has no\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/pandas/io/parsers/readers.py\u001b[0m in \u001b[0;36mread_csv\u001b[0;34m(filepath_or_buffer, sep, delimiter, header, names, index_col, usecols, squeeze, prefix, mangle_dupe_cols, dtype, engine, converters, true_values, false_values, skipinitialspace, skiprows, skipfooter, nrows, na_values, keep_default_na, na_filter, verbose, skip_blank_lines, parse_dates, infer_datetime_format, keep_date_col, date_parser, dayfirst, cache_dates, iterator, chunksize, compression, thousands, decimal, lineterminator, quotechar, quoting, doublequote, escapechar, comment, encoding, encoding_errors, dialect, error_bad_lines, warn_bad_lines, on_bad_lines, delim_whitespace, low_memory, memory_map, float_precision, storage_options)\u001b[0m\n\u001b[1;32m    948\u001b[0m     \u001b[0mkwds\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mupdate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkwds_defaults\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    949\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 950\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0m_read\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfilepath_or_buffer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    951\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    952\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/pandas/io/parsers/readers.py\u001b[0m in \u001b[0;36m_read\u001b[0;34m(filepath_or_buffer, kwds)\u001b[0m\n\u001b[1;32m    603\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    604\u001b[0m     \u001b[0;31m# Create the parser.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 605\u001b[0;31m     \u001b[0mparser\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mTextFileReader\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfilepath_or_buffer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    606\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    607\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mchunksize\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0miterator\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/pandas/io/parsers/readers.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, f, engine, **kwds)\u001b[0m\n\u001b[1;32m   1440\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1441\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mhandles\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mIOHandles\u001b[0m \u001b[0;34m|\u001b[0m \u001b[0;32mNone\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1442\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_engine\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_make_engine\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mf\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mengine\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1443\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1444\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mclose\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/pandas/io/parsers/readers.py\u001b[0m in \u001b[0;36m_make_engine\u001b[0;34m(self, f, engine)\u001b[0m\n\u001b[1;32m   1733\u001b[0m                 \u001b[0;32mif\u001b[0m \u001b[0;34m\"b\"\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mmode\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1734\u001b[0m                     \u001b[0mmode\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0;34m\"b\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1735\u001b[0;31m             self.handles = get_handle(\n\u001b[0m\u001b[1;32m   1736\u001b[0m                 \u001b[0mf\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1737\u001b[0m                 \u001b[0mmode\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/pandas/io/common.py\u001b[0m in \u001b[0;36mget_handle\u001b[0;34m(path_or_buf, mode, encoding, compression, memory_map, is_text, errors, storage_options)\u001b[0m\n\u001b[1;32m    854\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mioargs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mencoding\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0;34m\"b\"\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mioargs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmode\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    855\u001b[0m             \u001b[0;31m# Encoding\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 856\u001b[0;31m             handle = open(\n\u001b[0m\u001b[1;32m    857\u001b[0m                 \u001b[0mhandle\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    858\u001b[0m                 \u001b[0mioargs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmode\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: 'data.csv'"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "##Document classification"
      ],
      "metadata": {
        "id": "Eiegma7PoVMV"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "###SVM linear kernel"
      ],
      "metadata": {
        "id": "3suEqk9wojvj"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "##related to scaling and splitting\n",
        "\n",
        "y = df['y-axis-mean']\n",
        "df.drop(columns={'y-axis-mean'},inplace=True)\n",
        "x=df\n",
        "\n",
        "##personally have a feeling that minmax scaler would by far be the better one\n",
        "\n",
        "#x=MinMaxScaler().fit_transform(x)\n",
        "x=StandardScaler().fit_transform(x)\n",
        "X_Train , X_Test , Y_Train , Y_Test = tts(x,y,test_size = 0.15 , random_state = 42)"
      ],
      "metadata": {
        "id": "a7KYNar-wVqx"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "parameters = dict(kernel=['linear','rbf'],C = [1e-2,5*1e-2,1e-1,5*1e-1,1,5,10,50,100,500,1e3,5000,1e4,5*1e4,1e5,5*1e5,1e6,5*1e6,1e7], gamma=[0.1,0.01,0.001,0.0001,1e-4,1e-5,1e-6])\n"
      ],
      "metadata": {
        "id": "Dd-XiA0Zweay"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "####Preprocess data<br/>implement the cross validation function"
      ],
      "metadata": {
        "id": "JTNoSqp0ozKQ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "####Train and evaluation section"
      ],
      "metadata": {
        "id": "3-emcgyypLQ4"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "random_model = svm.SVC(max_iter=10000,probability=True,verbose=False)\n",
        "grid_random_model_acc = RandomizedSearchCV(estimator=random_model,param_distributions=parameters,n_iter=50,n_jobs=-1,cv=10,scoring='accuracy',refit=True)\n",
        "grid_random_model_f1 = RandomizedSearchCV(estimator=random_model,param_distributions=parameters,n_iter=50,n_jobs=-1,cv=10,scoring='f1',refit=True)\n",
        "grid_random_model_rac = RandomizedSearchCV(estimator=random_model,param_distributions=parameters,n_iter=50,n_jobs=-1,cv=10,scoring='roc_auc',refit=True)"
      ],
      "metadata": {
        "id": "KYU2QKCfw7jl"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(f'score of model with score set to accuracy :{grid_random_model_acc.best_score_}')\n",
        "print(f'parameters of model with score set to f1:{grid_random_model_f1.best_params_}')\n",
        "print(f'parameters of model with score set to roc_auc:{grid_random_model_rac.best_params_}')"
      ],
      "metadata": {
        "id": "XiHIbVjexAyN"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(f'score of model with score set to accuracy :{grid_random_model_acc.best_score_}')\n",
        "print(f'parameters of model with score set to accuracy:{grid_random_model_acc.best_params_}')\n",
        "print(f'score of model with score set to f1 :{grid_random_model_f1.best_score_}')\n",
        "print(f'parameters of model with score set to f1:{grid_random_model_f1.best_params_}')\n",
        "print(f'score of model with score set to roc-auc :{grid_random_model_rac.best_score_}')\n",
        "print(f'parameters of model with score set to roc-auc:{grid_random_model_rac.best_params_}')"
      ],
      "metadata": {
        "id": "X-LybhEkxr4P"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "##could become a separate function of roc curve\n",
        "\n",
        "\n",
        "y_score = modelAcc.predict_proba(x)[:, 1]\n",
        "fpr, tpr, thresholds = metrics.roc_curve(y, y_score)\n",
        "\n",
        "fig = px.area(\n",
        "    x=fpr, y=tpr,\n",
        "    title=f'ROC Curve (AUC={auc(fpr, tpr):.4f})  -- score:accuracy',\n",
        "    labels=dict(x='False Positive Rate', y='True Positive Rate'),\n",
        "    width=700, height=500\n",
        ")\n",
        "fig.add_shape(\n",
        "    type='line', line=dict(dash='dash'),\n",
        "    x0=0, x1=1, y0=0, y1=1\n",
        ")\n",
        "\n",
        "fig.update_yaxes(scaleanchor=\"x\", scaleratio=1)\n",
        "fig.update_xaxes(constrain='domain')\n",
        "fig.show()"
      ],
      "metadata": {
        "id": "Soa3p75tx0an"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "##could become a separate precision-recall curve function\n",
        "\n",
        "y_score = modelAcc.predict_proba(x)[:, 1]\n",
        "precision, recall, thresholds = precision_recall_curve(y, y_score)\n",
        "\n",
        "fig = px.area(\n",
        "    x=recall, y=precision,\n",
        "    title=f'Precision-Recall Curve (AUC={auc(fpr, tpr):.4f}) -- score : accuracy',\n",
        "    labels=dict(x='Recall', y='Precision'),\n",
        "    width=700, height=500\n",
        ")\n",
        "fig.add_shape(\n",
        "    type='line', line=dict(dash='dash'),\n",
        "    x0=0, x1=1, y0=1, y1=0\n",
        ")\n",
        "fig.update_yaxes(scaleanchor=\"x\", scaleratio=1)\n",
        "fig.update_xaxes(constrain='domain')\n",
        "\n",
        "fig.show()"
      ],
      "metadata": {
        "id": "tVeV9eI2x81A"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "##could become a separate function of learning curve (specifically for svm)\n",
        "\n",
        "train_sizes, train_scores, test_scores = learning_curve(estimator=modelAcc, X=x, y=y,cv=10,)\n",
        "train_mean = np.mean(train_scores, axis=1)\n",
        "train_std = np.std(train_scores, axis=1)\n",
        "test_mean = np.mean(test_scores, axis=1)\n",
        "test_std = np.std(test_scores, axis=1)\n",
        "\n",
        "plt.plot(train_sizes, train_mean, color='blue', marker='o', markersize=5, label='Training Accuracy')\n",
        "plt.fill_between(train_sizes, train_mean + train_std, train_mean - train_std, alpha=0.15, color='blue')\n",
        "plt.plot(train_sizes, test_mean, color='green', marker='+', markersize=5, linestyle='--', label='Validation Accuracy')\n",
        "plt.fill_between(train_sizes, test_mean + test_std, test_mean - test_std, alpha=0.15, color='green')\n",
        "plt.title('Learning Curve for modelAcc')\n",
        "plt.xlabel('Training Data Size')\n",
        "plt.ylabel('Model accuracy')\n",
        "plt.grid()\n",
        "plt.legend(loc='lower right')\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "pYQeziB0yK7W"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "####Test"
      ],
      "metadata": {
        "id": "Cz5jz2SFpQuW"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "###Transformer-based model"
      ],
      "metadata": {
        "id": "Zcnv5eAvomaU"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "beF-TcxQnlh2"
      },
      "outputs": [],
      "source": [
        "from transformers import BertConfig, BertTokenizer\n",
        "from transformers import BertModel\n",
        "\n",
        "from transformers import get_linear_schedule_with_warmup\n",
        "\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "import os"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "##split dataframe into train dev tet\n",
        "\n",
        "train, test = train_test_split(new_data, test_size=0.2, random_state=1, stratify=new_data['author'])\n",
        "train, valid = train_test_split(train, test_size=0.2, random_state=1, stratify=train['author'])\n",
        "\n",
        "train = train.reset_index(drop=True)\n",
        "valid = valid.reset_index(drop=True)\n",
        "test = test.reset_index(drop=True)\n",
        "\n",
        "x_train, y_train = train['normalized_sents'].values.tolist(), train['author'].values.tolist()\n",
        "x_valid, y_valid = valid['normalized_sents'].values.tolist(), valid['author'].values.tolist()\n",
        "x_test, y_test = test['normalized_sents'].values.tolist(), test['author'].values.tolist()\n",
        "\n",
        "print(train.shape)\n",
        "print(valid.shape)\n",
        "print(test.shape)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "qgpgqS690oCs",
        "outputId": "211e61a3-12da-4b2a-ef70-6680555e1596"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "(8662, 3)\n",
            "(2166, 3)\n",
            "(2708, 3)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SperdZDDWKxT"
      },
      "source": [
        "##### Configuration"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "WFpUoggdpU3x",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "3e4bf8c1-6958-4030-a80e-5024bc37ff56"
      },
      "source": [
        "##sit the model on the gpu\n",
        "\n",
        "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
        "print(f'device: {device}')\n",
        "\n",
        "train_on_gpu = torch.cuda.is_available()\n",
        "\n",
        "if not train_on_gpu:\n",
        "    print('CUDA is not available.  Training on CPU ...')\n",
        "else:\n",
        "    print('CUDA is available!  Training on GPU ...')"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "device: cpu\n",
            "CUDA is not available.  Training on CPU ...\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mH38OJU0X7rd"
      },
      "source": [
        "# general config\n",
        "##to be changed....\n",
        "\n",
        "\n",
        "MAX_LEN = 128\n",
        "TRAIN_BATCH_SIZE = 16\n",
        "VALID_BATCH_SIZE = 16\n",
        "TEST_BATCH_SIZE = 16\n",
        "\n",
        "EPOCHS = 3\n",
        "EEVERY_EPOCH = 1000\n",
        "LEARNING_RATE = 2e-5\n",
        "CLIP = 0.0\n",
        "\n",
        "MODEL_NAME_OR_PATH = 'bert-base-uncased'\n",
        "OUTPUT_PATH = '/auth_classification_bert/pytorch_model.bin'\n",
        "\n",
        "os.makedirs(os.path.dirname(OUTPUT_PATH), exist_ok=True)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qK02AC0pYIPj",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "a4e253d9-5ef7-430b-fee0-ecbe2f850bfb"
      },
      "source": [
        "# create a key finder based on label 2 id and id to label\n",
        "##tokenizing the labels in a handy way\n",
        "\n",
        "label2id = {label: i for i, label in enumerate(name_list)}\n",
        "id2label = {v: k for k, v in label2id.items()}\n",
        "\n",
        "print(f'label2id: {label2id}')\n",
        "print(f'id2label: {id2label}')"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "label2id: {'heg': 0, 'kan': 1, 'nei': 2}\n",
            "id2label: {0: 'heg', 1: 'kan', 2: 'nei'}\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qGJRNBXFYOcx",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 720,
          "referenced_widgets": [
            "377d8e293359446b99f3b7ea5bdc737d",
            "495c255035884aa5a8da622436236e58",
            "e0d82cfe3881405bad467225c133560d",
            "88a694d864b1433db7a645dd34275e45",
            "27d4ff12baf044929834f7130e9e36ba",
            "4ff8d0650648483faa202b3967900b28",
            "9a42d8d128e343febd16f8b8722ab8e9",
            "98be237063a2478e940195e444875227",
            "57617fb77add48a5a4c15abcd378337f",
            "73f37267f9c74138923c6817bc1395f7",
            "51928147e32b4574bcda4f0973e72d83",
            "a3551d28e040490db5ea870b1d948e5c",
            "3edcdab82973416496799c19df581c73",
            "54b1687fb0274dd8943def2266e34862",
            "a6f50e005b7f4d288b405691e7ff2fb3",
            "73f8cb3cd4f245ffb0e3e3db307c639b",
            "4d85d4abae9a48098d7daf8bda8f0f3f",
            "1e49e5eb5422429389424ef25115b33e",
            "31769353f33646c29bfddd3f2000c653",
            "3225458632e54955b3f9fd0159881557",
            "8b126a7ddfe84766a830af00a9598a34",
            "51909caf394d426eaa15244fb2f8e621",
            "4f727d83a9ef410cb74366ee098da5fe",
            "551f3b75e95343c2ab426f5b59221a85",
            "538036bf888d42c4a9e4097a17c06d0a",
            "c70af5437664409ab23b005ccb01dbd5",
            "415dbca238564a9baa4bd33c28334a5e",
            "4f6d1c7f49c740c49844c100dcecf637",
            "e9085f40d1e44b7f9099be07086aa6f7",
            "2749c634c75c45ea9cd33a9c5ac717e5",
            "d79501f98e3b426fb802ca06eac92713",
            "fffa1fc85e6e4a42a83d5ade2a7ca647",
            "c5afe3eda8d34e259dcb65d7d1d9a5d9"
          ]
        },
        "outputId": "dcf8ca5c-0bcc-491f-d237-c2c3e216d1f3"
      },
      "source": [
        "# setup the tokenizer and configuration\n",
        "\n",
        "tokenizer = BertTokenizer.from_pretrained(MODEL_NAME_OR_PATH)\n",
        "config = BertConfig.from_pretrained(\n",
        "    MODEL_NAME_OR_PATH, **{\n",
        "        'label2id': label2id,\n",
        "        'id2label': id2label,\n",
        "    })\n",
        "\n",
        "print(config.to_json_string())"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "Downloading (…)solve/main/vocab.txt:   0%|          | 0.00/232k [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "377d8e293359446b99f3b7ea5bdc737d"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "Downloading (…)okenizer_config.json:   0%|          | 0.00/28.0 [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "a3551d28e040490db5ea870b1d948e5c"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "Downloading (…)lve/main/config.json:   0%|          | 0.00/570 [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "4f727d83a9ef410cb74366ee098da5fe"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "{\n",
            "  \"architectures\": [\n",
            "    \"BertForMaskedLM\"\n",
            "  ],\n",
            "  \"attention_probs_dropout_prob\": 0.1,\n",
            "  \"classifier_dropout\": null,\n",
            "  \"gradient_checkpointing\": false,\n",
            "  \"hidden_act\": \"gelu\",\n",
            "  \"hidden_dropout_prob\": 0.1,\n",
            "  \"hidden_size\": 768,\n",
            "  \"id2label\": {\n",
            "    \"0\": \"heg\",\n",
            "    \"1\": \"kan\",\n",
            "    \"2\": \"nei\"\n",
            "  },\n",
            "  \"initializer_range\": 0.02,\n",
            "  \"intermediate_size\": 3072,\n",
            "  \"label2id\": {\n",
            "    \"heg\": 0,\n",
            "    \"kan\": 1,\n",
            "    \"nei\": 2\n",
            "  },\n",
            "  \"layer_norm_eps\": 1e-12,\n",
            "  \"max_position_embeddings\": 512,\n",
            "  \"model_type\": \"bert\",\n",
            "  \"num_attention_heads\": 12,\n",
            "  \"num_hidden_layers\": 12,\n",
            "  \"pad_token_id\": 0,\n",
            "  \"position_embedding_type\": \"absolute\",\n",
            "  \"transformers_version\": \"4.27.1\",\n",
            "  \"type_vocab_size\": 2,\n",
            "  \"use_cache\": true,\n",
            "  \"vocab_size\": 30522\n",
            "}\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cr9L9N91gSpm"
      },
      "source": [
        "##### Input Embeddings"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3BIajCqGgYEG",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "d7a2b0dd-dc98-4064-f7ba-b7e3e7211fd5"
      },
      "source": [
        "idx = np.random.randint(0, len(train))\n",
        "sample_sent = train.iloc[idx]['normalized_sents']\n",
        "sample_auth = train.iloc[idx]['author']\n",
        "\n",
        "print(f'Sample: \\n{\" \".join(sample_sent)}\\n{sample_auth}')"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Sample: \n",
            "proof one perhaps say immediately con scious representation external things consequently still remains undecided whether something outside corresponding\n",
            "kan\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ygnLJu8uhjPT",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "83a0803f-13f0-4039-d203-3f9ab224c48d"
      },
      "source": [
        "tokens = sample_sent\n",
        "token_ids = tokenizer.convert_tokens_to_ids(tokens)\n",
        "\n",
        "print(f'  sentence: {\" \".join(sample_sent)}')\n",
        "print(f'  Coded_tokens: {tokens}')\n",
        "print(f'   Tokens: {tokenizer.convert_tokens_to_string(tokens)}')\n",
        "print(f'Token IDs: {token_ids}')"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "  sentence: proof one perhaps say immediately con scious representation external things consequently still remains undecided whether something outside corresponding\n",
            "  Coded_tokens: ['proof', 'one', 'perhaps', 'say', 'immediately', 'con', 'scious', 'representation', 'external', 'things', 'consequently', 'still', 'remains', 'undecided', 'whether', 'something', 'outside', 'corresponding']\n",
            "   Tokens: proof one perhaps say immediately con scious representation external things consequently still remains undecided whether something outside corresponding\n",
            "Token IDs: [6947, 2028, 3383, 2360, 3202, 9530, 100, 6630, 6327, 2477, 8821, 2145, 3464, 100, 3251, 2242, 2648, 7978]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jgsgZ2b5h2I4",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "2a797527-dd2b-4556-b98a-c5bd1a3f7638"
      },
      "source": [
        "encoding = tokenizer.encode_plus(\n",
        "    sample_sent,\n",
        "    max_length=32,\n",
        "    truncation=True,\n",
        "    add_special_tokens=True, # Add '[CLS]' and '[SEP]'\n",
        "    return_token_type_ids=True,\n",
        "    return_attention_mask=True,\n",
        "    padding='max_length',\n",
        "    return_tensors='pt',  # Return PyTorch tensors\n",
        ")\n",
        "\n",
        "print(f'Keys: {encoding.keys()}\\n')\n",
        "for k in encoding.keys():\n",
        "    print(f'{k}:\\n{encoding[k]}')"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Keys: dict_keys(['input_ids', 'token_type_ids', 'attention_mask'])\n",
            "\n",
            "input_ids:\n",
            "tensor([[ 101, 6947, 2028, 3383, 2360, 3202, 9530,  100, 6630, 6327, 2477, 8821,\n",
            "         2145, 3464,  100, 3251, 2242, 2648, 7978,  102,    0,    0,    0,    0,\n",
            "            0,    0,    0,    0,    0,    0,    0,    0]])\n",
            "token_type_ids:\n",
            "tensor([[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
            "         0, 0, 0, 0, 0, 0, 0, 0]])\n",
            "attention_mask:\n",
            "tensor([[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0,\n",
            "         0, 0, 0, 0, 0, 0, 0, 0]])\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Pr8cRm9xiyKh"
      },
      "source": [
        "##### Dataset"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "TaJBSSuMizgr"
      },
      "source": [
        "class PhilDataset(torch.utils.data.Dataset):\n",
        "\n",
        "    def __init__(self, tokenizer, sent, targets=None, label_list=None, max_len=128):\n",
        "        self.sent = sent\n",
        "        self.targets = targets\n",
        "        self.has_target = isinstance(targets, list) or isinstance(targets, np.ndarray)\n",
        "\n",
        "        self.tokenizer = tokenizer\n",
        "        self.max_len = max_len\n",
        "\n",
        "\n",
        "        self.label_map = {label: i for i, label in enumerate(label_list)} if isinstance(label_list, list) else {}\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.sent)\n",
        "\n",
        "    def __getitem__(self, item):\n",
        "        sentence = str(self.sent[item])\n",
        "\n",
        "        if self.has_target:\n",
        "            target = self.targets[item]\n",
        "\n",
        "        encoding = self.tokenizer.encode_plus(\n",
        "            sentence,\n",
        "            add_special_tokens=True,\n",
        "            truncation=True,\n",
        "            max_length=self.max_len,\n",
        "            return_token_type_ids=True,\n",
        "            padding='max_length',\n",
        "            return_attention_mask=True,\n",
        "            return_tensors='pt')\n",
        "\n",
        "        inputs = {\n",
        "            'sentence': sentence,\n",
        "            'input_ids': encoding['input_ids'].flatten(),\n",
        "            'attention_mask': encoding['attention_mask'].flatten(),\n",
        "            'token_type_ids': encoding['token_type_ids'].flatten(),\n",
        "        }\n",
        "\n",
        "        if self.has_target:\n",
        "            inputs['targets'] = torch.tensor(target, dtype=torch.long)\n",
        "\n",
        "        return inputs\n",
        "\n",
        "\n",
        "def create_data_loader(x, y, tokenizer, max_len, batch_size, label_list):\n",
        "    dataset = PhilDataset(\n",
        "        sent=x,\n",
        "        targets=y,\n",
        "        tokenizer=tokenizer,\n",
        "        max_len=max_len,\n",
        "        label_list=label_list)\n",
        "\n",
        "    return torch.utils.data.DataLoader(dataset, batch_size=batch_size)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "JEcefj6fkZFl"
      },
      "source": [
        "label_list = name_list\n",
        "train_data_loader = create_data_loader(train['normalized_sents'].to_numpy(), train['labels'].to_numpy(), tokenizer, MAX_LEN, TRAIN_BATCH_SIZE, label_list)\n",
        "valid_data_loader = create_data_loader(valid['normalized_sents'].to_numpy(), valid['labels'].to_numpy(), tokenizer, MAX_LEN, VALID_BATCH_SIZE, label_list)\n",
        "test_data_loader = create_data_loader(test['normalized_sents'].to_numpy(), None, tokenizer, MAX_LEN, TEST_BATCH_SIZE, label_list)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_qSxzPU2krDk",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "5ec40e38-947e-4ecc-b161-6ebc12534227"
      },
      "source": [
        "sample_data = next(iter(train_data_loader))\n",
        "\n",
        "print(sample_data.keys())\n",
        "\n",
        "print(sample_data['sentence'])\n",
        "print(sample_data['input_ids'].shape)\n",
        "print(sample_data['input_ids'][0, :])\n",
        "print(sample_data['attention_mask'].shape)\n",
        "print(sample_data['attention_mask'][0, :])\n",
        "print(sample_data['token_type_ids'].shape)\n",
        "print(sample_data['token_type_ids'][0, :])\n",
        "print(sample_data['targets'].shape)\n",
        "print(sample_data['targets'])"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "dict_keys(['sentence', 'input_ids', 'attention_mask', 'token_type_ids', 'targets'])\n",
            "[\"['employ', 'innovations', 'quaint', 'old', 'terms', 'language', 'favour', 'rare', 'strange', 'concerned', 'aug\\\\xad', 'ment', 'vocabulary', 'rather', 'restrict', 'always', 'sign', 'immature', 'corrupted', 'taste']\", \"['notion', 'consciousness', 'forms', 'absolute', 'interfusion', 'individuality', 'let', 'see', 'whether', 'notion', 'confirmed', 'experience', 'whether', 'reality', 'corresponds']\", \"['one', 'indi\\\\xad', 'vidual', 'needs', 'health', 'another', 'cause', 'sickness', 'many', 'ways', 'means', 'freedom', 'spirit', 'may', 'highly', 'developed', 'natures', 'count', 'ways', 'means', 'unfreedom']\", \"['deceiving', 'priesthodd', 'dppressive', 'despot', 'therefore', 'ndt', 'directly', 'dbject', 'activity', 'object', 'insight', 'devoid']\", \"['individual', 'rules', 'life', 'excite', 'hostility', 'adopts', 'people', 'feel', 'humiliated', 'exceptional', 'treatment', 'accords', 'though', 'treated', 'merely', 'commonplace', 'creatures']\", \"['words', 'shape', 'yet', 'form', 'notion']\", \"['thought', 'highly', 'convictions', 'brought', 'sacrifices', 'every', 'kind', 'spared', 'honour', 'body', 'life', 'service', 'de\\\\xad', 'voted', 'half', 'energy', 'investigating', 'right', 'adhered', 'conviction', 'path', 'arrived', 'peaceable', 'picture', 'history', 'mankind', 'would', 'present']\", \"['never', 'get', 'crowd', 'cry', 'hosanna', 'ride', 'town', 'ass']\", \"['good', 'actions', 'sublimated', 'evil', 'ones', 'evil', 'actions', 'coarsened', 'brutalized', 'good', 'ones']\", \"['judging', 'consciousness', 'meant', 'quite', 'contrary']\", \"['second', 'principle', 'realities', 'mere', 'affirmations', 'never', 'log', 'ically', 'oppose', 'entirely', 'true', 'proposition', 'relaa', 'tions', 'concepts', 'signifies', 'nothing', 'either', 'regard', 'nature', 'overall', 'regard', 'anything', 'concept']\", \"['aware', 'objectively', 'real', 'world']\", \"['deal', 'unconditioned', 'causality', 'unconditioned', 'existence', 'substance']\", \"['one', 'absolute', 'pure', 'thinking', 'imme\\\\xad', 'diately', 'pure', 'consciousness', 'outside', 'finite', 'consciousness', 'negative', 'beyond']\", \"['use', 'notes', 'references', 'marginal', 'pag\\\\xad', 'ination', 'show', 'changes', 'made', 'first', 'second', 'editions']\", \"['although', 'general', 'logic', 'give', 'precepts', 'power', 'judgment', 'things', 'quite', 'different', 'transcendental', 'logic', 'even', 'seems', 'latter', 'proper', 'business', 'correct', 'secure', 'power', 'judgment', 'use', 'pure', 'understanding', 'determinate', 'rules']\"]\n",
            "torch.Size([16, 128])\n",
            "tensor([  101,  1031,  1005, 12666,  1005,  1010,  1005, 15463,  1005,  1010,\n",
            "         1005, 24209, 22325,  1005,  1010,  1005,  2214,  1005,  1010,  1005,\n",
            "         3408,  1005,  1010,  1005,  2653,  1005,  1010,  1005,  7927,  1005,\n",
            "         1010,  1005,  4678,  1005,  1010,  1005,  4326,  1005,  1010,  1005,\n",
            "         4986,  1005,  1010,  1005, 15476,  1032,  1060,  4215,  1005,  1010,\n",
            "         1005,  2273,  2102,  1005,  1010,  1005, 16188,  1005,  1010,  1005,\n",
            "         2738,  1005,  1010,  1005, 21573,  1005,  1010,  1005,  2467,  1005,\n",
            "         1010,  1005,  3696,  1005,  1010,  1005, 26838,  1005,  1010,  1005,\n",
            "        27279,  1005,  1010,  1005,  5510,  1005,  1033,   102,     0,     0,\n",
            "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
            "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
            "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
            "            0,     0,     0,     0,     0,     0,     0,     0])\n",
            "torch.Size([16, 128])\n",
            "tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
            "        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
            "        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
            "        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0,\n",
            "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
            "        0, 0, 0, 0, 0, 0, 0, 0])\n",
            "torch.Size([16, 128])\n",
            "tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
            "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
            "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
            "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
            "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
            "        0, 0, 0, 0, 0, 0, 0, 0])\n",
            "torch.Size([16])\n",
            "tensor([2, 0, 2, 0, 2, 0, 2, 2, 2, 0, 1, 0, 1, 0, 1, 1])\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "tDUNgRODuOTK",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "e2eb05f0-abbe-4db3-ce5e-d53907f4059f"
      },
      "source": [
        "sample_test = next(iter(test_data_loader))\n",
        "print(sample_test.keys())"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "dict_keys(['sentence', 'input_ids', 'attention_mask', 'token_type_ids'])\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Mv75ARn_R_Dt"
      },
      "source": [
        "class ClassifierModel(nn.Module):\n",
        "\n",
        "    def __init__(self, config):\n",
        "        super(ClassifierModel, self).__init__()\n",
        "\n",
        "        self.bert = BertModel.from_pretrained(MODEL_NAME_OR_PATH)\n",
        "        self.dropout = nn.Dropout(config.hidden_dropout_prob)\n",
        "        self.classifier = nn.Linear(config.hidden_size, 10)\n",
        "\n",
        "    def forward(self, input_ids, attention_mask, token_type_ids):\n",
        "        pooled_output = self.bert(\n",
        "            input_ids=input_ids,\n",
        "            attention_mask=attention_mask,\n",
        "            token_type_ids=token_type_ids)['pooler_output']\n",
        "\n",
        "        pooled_output = self.dropout(pooled_output)\n",
        "        logits = self.classifier(pooled_output)\n",
        "        return logits"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "VrObbZAdNTNl"
      },
      "source": [
        "import torch, gc\n",
        "\n",
        "gc.collect()\n",
        "torch.cuda.empty_cache()\n",
        "pt_model = None"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7vzQGZGUmw3-",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "f56207d4-0d60-4d2e-852a-7b300f703f3e"
      },
      "source": [
        "pt_model = ClassifierModel(config=config)\n",
        "pt_model = pt_model.to(device)\n",
        "\n",
        "print('pt_model', type(pt_model))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Some weights of the model checkpoint at bert-base-uncased were not used when initializing BertModel: ['cls.predictions.bias', 'cls.predictions.transform.dense.weight', 'cls.predictions.transform.LayerNorm.weight', 'cls.seq_relationship.weight', 'cls.seq_relationship.bias', 'cls.predictions.decoder.weight', 'cls.predictions.transform.dense.bias', 'cls.predictions.transform.LayerNorm.bias']\n",
            "- This IS expected if you are initializing BertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
            "- This IS NOT expected if you are initializing BertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "pt_model <class '__main__.ClassifierModel'>\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "for name,param in pt_model.named_parameters():\n",
        "    print(name)"
      ],
      "metadata": {
        "id": "Lr_-sMgyTNfZ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tFZQDfLlp0Sf"
      },
      "source": [
        "##### Training"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.metrics import classification_report\n",
        "from sklearn.metrics import f1_score\n",
        "from sklearn.utils import shuffle\n",
        "\n",
        "import plotly.express as px\n",
        "import plotly.graph_objects as go\n",
        "\n",
        "from tqdm.notebook import tqdm\n",
        "\n",
        "import os\n",
        "\n",
        "import copy\n",
        "import collections\n",
        "import torch"
      ],
      "metadata": {
        "id": "o3peRYrH4ykA"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "e044fZSfBoKe"
      },
      "source": [
        "def simple_accuracy(y_true, y_pred):\n",
        "    return (y_true == y_pred).mean()\n",
        "\n",
        "def acc_and_f1(y_true, y_pred, average='weighted'):\n",
        "    acc = simple_accuracy(y_true, y_pred)\n",
        "    f1 = f1_score(y_true=y_true, y_pred=y_pred, average=average)\n",
        "    return {\n",
        "        \"acc\": acc,\n",
        "        \"f1\": f1,\n",
        "    }\n",
        "\n",
        "def y_loss(y_true, y_pred, losses):\n",
        "    y_true = torch.stack(y_true).cpu().detach().numpy()\n",
        "    y_pred = torch.stack(y_pred).cpu().detach().numpy()\n",
        "    y = [y_true, y_pred]\n",
        "    loss = np.mean(losses)\n",
        "\n",
        "    return y, loss\n",
        "\n",
        "\n",
        "def eval_op(model, data_loader, loss_fn):\n",
        "    model.eval()\n",
        "\n",
        "    losses = []\n",
        "    y_pred = []\n",
        "    y_true = []\n",
        "\n",
        "    with torch.no_grad():\n",
        "        for dl in tqdm(data_loader, total=len(data_loader), desc=\"Evaluation... \"):\n",
        "\n",
        "            input_ids = dl['input_ids']\n",
        "            attention_mask = dl['attention_mask']\n",
        "            token_type_ids = dl['token_type_ids']\n",
        "            targets = dl['targets']\n",
        "\n",
        "            # move tensors to GPU if CUDA is available\n",
        "            input_ids = input_ids.to(device)\n",
        "            attention_mask = attention_mask.to(device)\n",
        "            token_type_ids = token_type_ids.to(device)\n",
        "            targets = targets.to(device)\n",
        "\n",
        "            # compute predicted outputs by passing inputs to the model\n",
        "            outputs = model(\n",
        "                input_ids=input_ids,\n",
        "                attention_mask=attention_mask,\n",
        "                token_type_ids=token_type_ids)\n",
        "\n",
        "            # convert output probabilities to predicted class\n",
        "            _, preds = torch.max(outputs, dim=1)\n",
        "\n",
        "            # calculate the batch loss\n",
        "            loss = loss_fn(outputs, targets)\n",
        "\n",
        "            # accumulate all the losses\n",
        "            losses.append(loss.item())\n",
        "\n",
        "            y_pred.extend(preds)\n",
        "            y_true.extend(targets)\n",
        "\n",
        "    eval_y, eval_loss = y_loss(y_true, y_pred, losses)\n",
        "    return eval_y, eval_loss\n",
        "\n",
        "\n",
        "def train_op(model,\n",
        "             data_loader,\n",
        "             loss_fn,\n",
        "             optimizer,\n",
        "             scheduler,\n",
        "             step=0,\n",
        "             print_every_step=100,\n",
        "             eval=False,\n",
        "             eval_cb=None,\n",
        "             eval_loss_min=np.Inf,\n",
        "             eval_data_loader=None,\n",
        "             clip=0.0):\n",
        "\n",
        "    model.train()\n",
        "\n",
        "    losses = []\n",
        "    y_pred = []\n",
        "    y_true = []\n",
        "\n",
        "    for dl in tqdm(data_loader, total=len(data_loader), desc=\"Training... \"):\n",
        "        step += 1\n",
        "\n",
        "        input_ids = dl['input_ids']\n",
        "        attention_mask = dl['attention_mask']\n",
        "        token_type_ids = dl['token_type_ids']\n",
        "        targets = dl['targets']\n",
        "\n",
        "        # move tensors to GPU if CUDA is available\n",
        "        input_ids = input_ids.to(device)\n",
        "        attention_mask = attention_mask.to(device)\n",
        "        token_type_ids = token_type_ids.to(device)\n",
        "        targets = targets.to(device)\n",
        "\n",
        "        # clear the gradients of all optimized variables\n",
        "        optimizer.zero_grad()\n",
        "\n",
        "        # compute predicted outputs by passing inputs to the model\n",
        "        outputs = model(\n",
        "            input_ids=input_ids,\n",
        "            attention_mask=attention_mask,\n",
        "            token_type_ids=token_type_ids)\n",
        "\n",
        "        # convert output probabilities to predicted class\n",
        "        _, preds = torch.max(outputs, dim=1)\n",
        "\n",
        "        # calculate the batch loss\n",
        "        loss = loss_fn(outputs, targets)\n",
        "\n",
        "        # accumulate all the losses\n",
        "        losses.append(loss.item())\n",
        "\n",
        "        # compute gradient of the loss with respect to model parameters\n",
        "        loss.backward()\n",
        "\n",
        "        # `clip_grad_norm` helps prevent the exploding gradient problem in RNNs / LSTMs.\n",
        "        if clip > 0.0:\n",
        "            nn.utils.clip_grad_norm_(model.parameters(), max_norm=clip)\n",
        "\n",
        "        # perform optimization step\n",
        "        optimizer.step()\n",
        "\n",
        "        # perform scheduler step\n",
        "        scheduler.step()\n",
        "\n",
        "        y_pred.extend(preds)\n",
        "        y_true.extend(targets)\n",
        "\n",
        "        if eval:\n",
        "            train_y, train_loss = y_loss(y_true, y_pred, losses)\n",
        "            train_score = acc_and_f1(train_y[0], train_y[1], average='weighted')\n",
        "\n",
        "            if step % print_every_step == 0:\n",
        "                eval_y, eval_loss = eval_op(model, eval_data_loader, loss_fn)\n",
        "                eval_score = acc_and_f1(eval_y[0], eval_y[1], average='weighted')\n",
        "\n",
        "                if hasattr(eval_cb, '__call__'):\n",
        "                    eval_loss_min = eval_cb(model, step, train_score, train_loss, eval_score, eval_loss, eval_loss_min)\n",
        "\n",
        "    train_y, train_loss = y_loss(y_true, y_pred, losses)\n",
        "\n",
        "    return train_y, train_loss, step, eval_loss_min\n",
        "\n",
        "def eval_callback(epoch, epochs, output_path):\n",
        "    def eval_cb(model, step, train_score, train_loss, eval_score, eval_loss, eval_loss_min):\n",
        "        statement = ''\n",
        "        statement += 'Epoch: {}/{}...'.format(epoch, epochs)\n",
        "        statement += 'Step: {}...'.format(step)\n",
        "\n",
        "        statement += 'Train Loss: {:.6f}...'.format(train_loss)\n",
        "        statement += 'Train Acc: {:.3f}...'.format(train_score['acc'])\n",
        "\n",
        "        statement += 'Valid Loss: {:.6f}...'.format(eval_loss)\n",
        "        statement += 'Valid Acc: {:.3f}...'.format(eval_score['acc'])\n",
        "\n",
        "        print(statement)\n",
        "\n",
        "        if eval_loss <= eval_loss_min:\n",
        "            print('Validation loss decreased ({:.6f} --> {:.6f}).  Saving model ...'.format(\n",
        "                eval_loss_min,\n",
        "                eval_loss))\n",
        "\n",
        "            torch.save(model.state_dict(), output_path)\n",
        "            eval_loss_min = eval_loss\n",
        "\n",
        "        return eval_loss_min\n",
        "\n",
        "\n",
        "    return eval_cb"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wTWrdialDAtN",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 81,
          "referenced_widgets": [
            "7e1f4837ac5542b69ba1f1fe3ecc1e9e",
            "5f53d3910ccb4ed8852f48567728122d",
            "f4b79e679e3b4116a7dc0cf85b5bbd79",
            "342df4eaabe543338af304faa3d8f945",
            "1d4b516f7e2843b29c1f294d19c497d1",
            "c973e17cc29f470aad4f88d9a1a03e0e",
            "ab97eedadcd54386a3a8ffbb8a4616b5",
            "aa938851d687499ba3fe97abedb127eb",
            "b72466527eac410896e3a2b70084898c",
            "a35c73fb2bbd45de9be4e42e36f07240",
            "bfb4ea4aeae04b7193c2949162aa01d2",
            "6713ec0510974eb2bba7bb755481b69d",
            "6736652ca7f24dd5a7b954ef76314d72",
            "a2ac6446c55d497d90b08a802ba8d3ef",
            "4b177dbcb3e741858fba042dd22c3f8a",
            "4938745fcc7644d3864ff21d32680d85",
            "f2491f9fd8474439ae14822376e1a677",
            "ddff20efa3dd4081be9f283c68cd4c1f",
            "a536b427160146319753fb221683caa7",
            "435355c2d03247df96f071b1cff5596a",
            "b8c7771a6cc848759a01d959a54bb028",
            "8ca95e8364d34d339c5c872fd988a92f"
          ]
        },
        "outputId": "723b03fd-83be-49ba-cc57-c3cb10482928"
      },
      "source": [
        "from torch import optim\n",
        "\n",
        "optimizer = optim.Adam(pt_model.parameters(), lr=LEARNING_RATE)\n",
        "total_steps = len(train_data_loader) * EPOCHS\n",
        "scheduler = get_linear_schedule_with_warmup(\n",
        "    optimizer,\n",
        "    num_warmup_steps=0,\n",
        "    num_training_steps=total_steps\n",
        ")\n",
        "\n",
        "loss_fn = nn.CrossEntropyLoss()\n",
        "\n",
        "step = 0\n",
        "eval_loss_min = np.Inf\n",
        "history = collections.defaultdict(list)\n",
        "\n",
        "\n",
        "for epoch in tqdm(range(1, EPOCHS + 1), desc=\"Epochs... \"):\n",
        "    train_y, train_loss, step, eval_loss_min = train_op(\n",
        "        model=pt_model,\n",
        "        data_loader=train_data_loader,\n",
        "        loss_fn=loss_fn,\n",
        "        optimizer=optimizer,\n",
        "        scheduler=scheduler,\n",
        "        step=step,\n",
        "        print_every_step=EEVERY_EPOCH,\n",
        "        eval=True,\n",
        "        eval_cb=eval_callback(epoch, EPOCHS, OUTPUT_PATH),\n",
        "        eval_loss_min=eval_loss_min,\n",
        "        eval_data_loader=valid_data_loader,\n",
        "        clip=CLIP)\n",
        "\n",
        "    train_score = acc_and_f1(train_y[0], train_y[1], average='weighted')\n",
        "\n",
        "    eval_y, eval_loss = eval_op(\n",
        "        model=pt_model,\n",
        "        data_loader=valid_data_loader,\n",
        "        loss_fn=loss_fn)\n",
        "\n",
        "    eval_score = acc_and_f1(eval_y[0], eval_y[1], average='weighted')\n",
        "\n",
        "    history['train_acc'].append(train_score['acc'])\n",
        "    history['train_loss'].append(train_loss)\n",
        "    history['val_acc'].append(eval_score['acc'])\n",
        "    history['val_loss'].append(eval_loss)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "Epochs... :   0%|          | 0/3 [00:00<?, ?it/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "7e1f4837ac5542b69ba1f1fe3ecc1e9e"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "Training... :   0%|          | 0/542 [00:00<?, ?it/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "6713ec0510974eb2bba7bb755481b69d"
            }
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "C-ZvVuRsoYRK"
      },
      "source": [
        "##### Prediction"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dlpDbg0wDqKP"
      },
      "source": [
        "def predict(model, comments, tokenizer, max_len=128, batch_size=32):\n",
        "    data_loader = create_data_loader(comments, None, tokenizer, max_len, batch_size, None)\n",
        "\n",
        "    predictions = []\n",
        "    prediction_probs = []\n",
        "\n",
        "\n",
        "    model.eval()\n",
        "    with torch.no_grad():\n",
        "        for dl in tqdm(data_loader, position=0):\n",
        "            input_ids = dl['input_ids']\n",
        "            attention_mask = dl['attention_mask']\n",
        "            token_type_ids = dl['token_type_ids']\n",
        "\n",
        "            # move tensors to GPU if CUDA is available\n",
        "            input_ids = input_ids.to(device)\n",
        "            attention_mask = attention_mask.to(device)\n",
        "            token_type_ids = token_type_ids.to(device)\n",
        "\n",
        "            # compute predicted outputs by passing inputs to the model\n",
        "            outputs = model(\n",
        "                input_ids=input_ids,\n",
        "                attention_mask=attention_mask,\n",
        "                token_type_ids=token_type_ids)\n",
        "\n",
        "            # convert output probabilities to predicted class\n",
        "            _, preds = torch.max(outputs, dim=1)\n",
        "\n",
        "            predictions.extend(preds)\n",
        "            prediction_probs.extend(F.softmax(outputs, dim=1))\n",
        "\n",
        "    predictions = torch.stack(predictions).cpu().detach().numpy()\n",
        "    prediction_probs = torch.stack(prediction_probs).cpu().detach().numpy()\n",
        "\n",
        "    return predictions, prediction_probs"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "RRpWTfwdoWoS",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 66,
          "referenced_widgets": [
            "ed41d53c9c2d4293847d6cba92678bc6",
            "b4c0de9cad2c4c51b99e7312af80f309",
            "01e9320dfc554ce4962e307bd46f719d",
            "bc5e097111d942e3ba5e8d5b3e1fecdf",
            "03c2c140d77d4e5b8682d993d5b0bf27",
            "36f5270be00d4316a4761c01b7df2c8e",
            "df6d404c345345548f615b8143ea3892",
            "66d4b5c390204095a6f3177a1d198041",
            "dab64da309014ca1a57bade6d1c55e5e",
            "293931024d0c4dd7a48720fafe0ce7c2",
            "8158df6c3dc3421592b6b248040a3fa5"
          ]
        },
        "outputId": "9fb11768-a77c-4159-c48d-46325a57d33d"
      },
      "source": [
        "test_sents = test['normalized_sents'].to_numpy()\n",
        "preds, probs = predict(pt_model, test_sents, tokenizer, max_len=128)\n",
        "\n",
        "print(preds.shape, probs.shape)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "  0%|          | 0/590 [00:00<?, ?it/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "ed41d53c9c2d4293847d6cba92678bc6"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "(18860,) (18860, 10)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "preds"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "VNlmNh7jazk7",
        "outputId": "12e6e1b1-1d6b-4075-954d-5b5faa01dea7"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([9, 5, 5, ..., 3, 4, 6])"
            ]
          },
          "metadata": {},
          "execution_count": 197
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "preds_label = [id2label[t] for t in preds]\n",
        "preds_label[0:5]"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "PD-if3dzbK69",
        "outputId": "7d779734-a6e7-4093-b57a-b716aa31cafd"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['parvin', 'hafez', 'hafez', 'asad', 'asadi']"
            ]
          },
          "metadata": {},
          "execution_count": 199
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#####F1 Score and report"
      ],
      "metadata": {
        "id": "Vzor4JVfYv38"
      }
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ZRL2bgDDpUG0",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "57f223cd-44e4-4e80-bcf0-8cd66fb03fd3"
      },
      "source": [
        "y_test, y_pred = test['author'].values, preds\n",
        "\n",
        "print(f'F1: {f1_score(y_test, y_pred, average=\"weighted\")}')\n",
        "print(\"--------------classification_report---------------\")\n",
        "print(classification_report(y_test, y_pred, target_names=label_list))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "F1: 0.18183054145230343\n",
            "--------------classification_report---------------\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "      khajoo       0.19      0.06      0.09      1886\n",
            "        asad       0.17      0.23      0.19      1886\n",
            "       bahar       0.18      0.21      0.19      1886\n",
            "        feyz       0.20      0.20      0.20      1886\n",
            "       asadi       0.28      0.41      0.33      1886\n",
            "       hafez       0.18      0.26      0.21      1886\n",
            "        jami       0.15      0.17      0.16      1886\n",
            "       kamal       0.16      0.12      0.14      1886\n",
            "     moulavi       0.22      0.09      0.13      1886\n",
            "      parvin       0.18      0.17      0.17      1886\n",
            "\n",
            "    accuracy                           0.19     18860\n",
            "   macro avg       0.19      0.19      0.18     18860\n",
            "weighted avg       0.19      0.19      0.18     18860\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.metrics import confusion_matrix\n",
        "print(confusion_matrix(y_test,y_pred))"
      ],
      "metadata": {
        "id": "d7ZQQDSDd1Iu",
        "outputId": "aa8903c4-4907-43c1-f369-635dd62886c2",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[[108 230 135 240 211 368 175 178 102 139]\n",
            " [ 42 427 178 172 363 210 164 122  41 167]\n",
            " [ 28 186 396 131 281 199 247 122  90 206]\n",
            " [ 61 231 157 370  91 361 230 155  83 147]\n",
            " [ 23 258 233  63 769  90 154 103  29 164]\n",
            " [ 79 231 181 217 104 495 202 149  80 148]\n",
            " [ 52 200 255 146 294 210 317 132  70 210]\n",
            " [ 74 261 182 222 209 282 201 229  86 140]\n",
            " [ 34 281 270 161 189 241 237 149 178 146]\n",
            " [ 53 242 263 138 199 292 202 131  48 318]]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "####Preprocess data(add token, tokenizaiton, padding)"
      ],
      "metadata": {
        "id": "ZU6dgtmppQIp"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "####Train and evaluation section"
      ],
      "metadata": {
        "id": "KTR8U0QgpgwV"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "####Test"
      ],
      "metadata": {
        "id": "KWTr0ohHpi_T"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##Token classification"
      ],
      "metadata": {
        "id": "rkpVHmoAofvI"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "###GRU and LSTM"
      ],
      "metadata": {
        "id": "byjufE1JoqdE"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class LSTMClassifier(nn.Module):\n",
        "\n",
        "    def __init__(self,\n",
        "        num_classes,\n",
        "        batch_size=10,\n",
        "        embedding_dim=100,\n",
        "        hidden_dim=50,\n",
        "        vocab_size=128):\n",
        "\n",
        "        super(LSTMClassifier, self).__init__()\n",
        "\n",
        "        initrange = 0.1\n",
        "\n",
        "        self.num_labels = num_classes\n",
        "        n = len(self.num_labels)\n",
        "        self.hidden_dim = hidden_dim\n",
        "        self.batch_size = batch_size\n",
        "\n",
        "        self.word_embeddings = nn.Embedding(vocab_size, embedding_dim)\n",
        "        self.word_embeddings.weight.data.uniform_(-initrange, initrange)\n",
        "        self.lstm = nn.LSTM(input_size=embedding_dim, hidden_size=hidden_dim, batch_first=True, bidirectional=True)  # !\n",
        "\n",
        "        print(\"# !\")\n",
        "\n",
        "        bi_grus = torch.nn.GRU(input_size=embedding_dim, hidden_size=hidden_dim, batch_first=True, bidirectional=True)\n",
        "        reverse_gru = torch.nn.GRU(input_size=embedding_dim, hidden_size=hidden_dim, batch_first=True, bidirectional=False)\n",
        "\n",
        "        self.lstm.weight_ih_l0_reverse = bi_grus.weight_ih_l0_reverse\n",
        "        self.lstm.weight_hh_l0_reverse = bi_grus.weight_hh_l0_reverse\n",
        "        self.lstm.bias_ih_l0_reverse = bi_grus.bias_ih_l0_reverse\n",
        "        self.lstm.bias_hh_l0_reverse = bi_grus.bias_hh_l0_reverse\n",
        "\n",
        "        bi_output, bi_hidden = bi_grus()\n",
        "        reverse_output, reverse_hidden = reverse_gru()\n",
        "\n",
        "        print(\"# !\")\n",
        "\n",
        "        # self.classifier = nn.Linear(hidden_dim, self.num_labels[0])\n",
        "        self.classifier = nn.Linear(2 * hidden_dim, self.num_labels[0])  # !\n",
        "\n",
        "\n",
        "    def repackage_hidden(h):\n",
        "        \"\"\"Wraps hidden states in new Tensors, to detach them from their history.\"\"\"\n",
        "\n",
        "        if isinstance(h, torch.Tensor):\n",
        "            return h.detach()\n",
        "        else:\n",
        "            return tuple(repackage_hidden(v) for v in h)\n",
        "\n",
        "\n",
        "    def forward(self, sentence, labels=None):\n",
        "        embeds = self.word_embeddings(sentence)\n",
        "        lstm_out, _ = self.lstm(embeds)  # lstm_out - 2 tensors, _ - hidden layer\n",
        "        print(lstm_out[:,-1,:])\n",
        "        tag_space = self.classifier(lstm_out[:,-1,:] + lstm_out[:,-1,:])  # !  # lstm_out[:,-1,:] - 1 tensor\n",
        "        logits = F.log_softmax(tag_space, dim=1)\n",
        "        loss = None\n",
        "        if labels:\n",
        "            loss = F.cross_entropy(logits.view(-1, self.num_labels[0]), labels[0].view(-1))\n",
        "        return loss, logits"
      ],
      "metadata": {
        "id": "7_aiplY5vSDm"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "###Transformer-based model"
      ],
      "metadata": {
        "id": "erIlhAL1otbf"
      }
    }
  ]
}